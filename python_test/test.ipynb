{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e1cc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tree_sitter import Language, Parser\n",
    "import tree_sitter_python, tree_sitter_javascript\n",
    "import os\n",
    "import json\n",
    "from helix import Client\n",
    "\n",
    "PY_LANGUAGE = Language(tree_sitter_python.language())\n",
    "JS_LANGUAGE = Language(tree_sitter_javascript.language())\n",
    "\n",
    "py_parser = Parser(PY_LANGUAGE)\n",
    "js_parser = Parser(JS_LANGUAGE)\n",
    "\n",
    "client = Client(local=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f048acbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file(file_path, parser):\n",
    "    \"\"\"Parse a single Python file and return its syntax tree.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            source_code = file.read()\n",
    "\n",
    "        return parser.parse(source_code), source_code\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing {file_path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def node_to_dict(node, source_code, order:int=1):\n",
    "    return {\n",
    "        \"type\": node.type,\n",
    "        \"start_byte\": node.start_byte,\n",
    "        \"end_byte\": node.end_byte,\n",
    "        \"order\": order,\n",
    "        \"text\": source_code[node.start_byte:node.end_byte].decode('utf8'),\n",
    "        \"children\": [node_to_dict(child, source_code, i+1) for i, child in enumerate(node.children)]\n",
    "    }\n",
    "\n",
    "def scan_directory(root_path):\n",
    "    folders = []\n",
    "    files = []\n",
    "\n",
    "    for entry in os.listdir(root_path):\n",
    "        full_path = os.path.join(root_path, entry)\n",
    "        if os.path.isdir(full_path):\n",
    "            folders.append(entry)\n",
    "        elif os.path.isfile(full_path):\n",
    "            files.append(entry)\n",
    "\n",
    "    return {\n",
    "        \"root\": root_path,\n",
    "        \"folders\": folders,\n",
    "        \"files\": files\n",
    "    }\n",
    "\n",
    "def process_path(path: str, code_base: dict):\n",
    "    dir_dict = scan_directory(path)\n",
    "    current_level = code_base\n",
    "    \n",
    "    path_parts = [p for p in path.split(os.sep) if p]\n",
    "    \n",
    "    # Navigate to the correct level in the dictionary\n",
    "    for part in path_parts:\n",
    "        if part not in current_level:\n",
    "            current_level[part] = {}\n",
    "        current_level = current_level[part]\n",
    "    \n",
    "    # Process files\n",
    "    for file in dir_dict[\"files\"]:\n",
    "        if file.endswith('.py'):\n",
    "            file_path = os.path.join(path, file)\n",
    "            tree, code = parse_file(file_path, py_parser)\n",
    "            if tree:\n",
    "                tree_dict = node_to_dict(tree.root_node, code, 1)\n",
    "                current_level[file] = tree_dict\n",
    "        else:\n",
    "            current_level[file] = \"file_content\"\n",
    "        del tree\n",
    "        del code\n",
    "        del tree_dict\n",
    "    \n",
    "    # Process subdirectories\n",
    "    for folder in dir_dict[\"folders\"]:\n",
    "        folder_path = os.path.join(path, folder)\n",
    "        if folder not in current_level:\n",
    "            current_level[folder] = {}\n",
    "        process_path(folder_path, code_base)\n",
    "    \n",
    "    return code_base\n",
    "\n",
    "# Initialize and run\n",
    "code_base = {}\n",
    "test_dict = process_path(\"sample\", code_base)\n",
    "test_dict['sample']['lab00.py']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b34bc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test.json\", \"w\") as f:\n",
    "    test_json = json.dump(test_dict, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840cced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_directory(root_path):\n",
    "    folders = []\n",
    "    files = []\n",
    "\n",
    "    for entry in os.listdir(root_path):\n",
    "        full_path = os.path.join(root_path, entry)\n",
    "        if os.path.isdir(full_path):\n",
    "            folders.append(entry)\n",
    "        elif os.path.isfile(full_path):\n",
    "            files.append(entry)\n",
    "\n",
    "    return {\n",
    "        \"folders\": folders,\n",
    "        \"files\": files\n",
    "    }\n",
    "\n",
    "def process_entities(parent_dict, parent_id):\n",
    "    children = parent_dict['children']\n",
    "    for entity in children:\n",
    "        # Create sub entity\n",
    "        entity_id = client.query('createSubEntity', {'entity_id': parent_id, 'entity_type': entity['type'], 'start_byte': entity['start_byte'], 'end_byte': entity['end_byte'], 'order': entity['order'], 'text': entity['text']})[0]['entity'][0]['id']\n",
    "        \n",
    "        # Recursively process sub entities in the entity\n",
    "        process_entities(entity, entity_id)\n",
    "\n",
    "def chunk_entity(text:str):\n",
    "    return [text[i:i+50] for i in range(0, len(text)+1, 50)]\n",
    "\n",
    "from random import random\n",
    "def random_embedding(text:str):\n",
    "    return [random() for _ in range(768)]\n",
    "\n",
    "def populate(full_path, curr_type='root', parent_id=None):\n",
    "    dir_dict = scan_directory(full_path)\n",
    "\n",
    "    print(f'Processing {len(dir_dict[\"folders\"])} folders')\n",
    "    for folder in dir_dict[\"folders\"]:\n",
    "        print(f\"Reached {folder}\")\n",
    "        if curr_type == 'root':\n",
    "            # Create super folder\n",
    "            folder_id = client.query('createSuperFolder', {'root_id': parent_id, 'name': folder})[0]['folder'][0]['id']\n",
    "        else:\n",
    "            # Create sub folder\n",
    "            folder_id = client.query('createSubFolder', {'folder_id': parent_id, 'name': folder})[0]['subfolder'][0]['id']\n",
    "        \n",
    "        # Recursively populate the stuff in the folder\n",
    "        populate(os.path.join(full_path, folder), curr_type='folder', parent_id=folder_id)\n",
    "\n",
    "    print(f'Processing {len(dir_dict[\"files\"])} files')\n",
    "    for file in dir_dict[\"files\"]:\n",
    "        print(f\"Reached {file}\")\n",
    "        if file.endswith('.py'):\n",
    "            # Extract python code structure with tree-sitter\n",
    "            file_path = os.path.join(full_path, file)\n",
    "            tree, code = parse_file(file_path, py_parser)\n",
    "\n",
    "            if tree:\n",
    "                tree_dict = node_to_dict(tree.root_node, code, 0)\n",
    "                del tree\n",
    "                del code\n",
    "\n",
    "                if curr_type == 'root':\n",
    "                    # Create super file\n",
    "                    file_id = client.query('createSuperFile', {'root_id': parent_id, 'name': file, 'text': tree_dict['text']})[0]['file'][0]['id']\n",
    "                else:\n",
    "                    # Create sub file\n",
    "                    file_id = client.query('createFile', {'folder_id': parent_id, 'name': file, 'text': tree_dict['text']})[0]['file'][0]['id']\n",
    "\n",
    "                children = tree_dict['children']\n",
    "                del tree_dict\n",
    "\n",
    "                print(f\"Process {len(children)} super entities\")\n",
    "                for superentity in children:\n",
    "                    print(f\"Reached {superentity['type']}\")\n",
    "                    # Create super entity\n",
    "                    super_entity_id = client.query('createSuperEntity', {'file_id': file_id, 'entity_type': superentity['type'], 'start_byte': superentity['start_byte'], 'end_byte': superentity['end_byte'], 'order': superentity['order'], 'text': superentity['text']})[0]['entity'][0]['id']\n",
    "                    \n",
    "                    # Embed super entity\n",
    "                    chunks = chunk_entity(superentity['text'])\n",
    "                    for chunk in chunks:\n",
    "                        client.query('embedSuperEntity', {'entity_id':super_entity_id, 'vector': random_embedding(chunk)})\n",
    "                        del chunk\n",
    "\n",
    "                    del chunks\n",
    "\n",
    "                    process_entities(superentity, super_entity_id)\n",
    "                    \n",
    "                    del superentity\n",
    "\n",
    "                del children\n",
    "            else:\n",
    "                print(f'Failed to parse file: {file}')\n",
    "                del tree\n",
    "                del code\n",
    "        else:\n",
    "            print(f'Not python file: {file}')\n",
    "\n",
    "    del dir_dict\n",
    "\n",
    "\n",
    "root_name = os.getcwd()\n",
    "root_id = client.query('createRoot', {'name': root_name})[0]['root'][0]['id']\n",
    "populate(root_name, parent_id=root_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c6185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gitignore():\n",
    "    to_ignore = {'files': [], 'folders': []}\n",
    "    with open('.gitignore', 'r') as f:\n",
    "        lines = f.read()\n",
    "    print(lines)\n",
    "\n",
    "    return to_ignore\n",
    "\n",
    "check_gitignore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3def4f69",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75f47f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from tree_sitter import Language, Parser\n",
    "import tree_sitter_python\n",
    "\n",
    "\n",
    "PY_LANGUAGE = Language(tree_sitter_python.language())\n",
    "py_parser = Parser(PY_LANGUAGE)\n",
    "\n",
    "grammar = json.loads(Path(\"node-types.json\").read_text())\n",
    "grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c8034e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = ['module', 'block']\n",
    "\n",
    "for parent in grammar:\n",
    "    if 'subtypes' in parent:\n",
    "        nodes.append(parent['type'])\n",
    "        if parent['type'] == '_compound_statement':\n",
    "            for child in parent['subtypes']:\n",
    "                nodes.append(child['type'])\n",
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f43d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [\n",
    "        \"module\",\n",
    "        \"block\",\n",
    "        \"_compound_statement\",\n",
    "        \"class_definition\",\n",
    "        \"decorated_definition\",\n",
    "        \"for_statement\",\n",
    "        \"function_definition\",\n",
    "        \"if_statement\",\n",
    "        \"match_statement\",\n",
    "        \"try_statement\",\n",
    "        \"while_statement\",\n",
    "        \"with_statement\",\n",
    "        \"_simple_statement\",\n",
    "        \"expression\",\n",
    "        \"parameter\",\n",
    "        \"pattern\",\n",
    "        \"primary_expression\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b830f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_to_dict(node, source_code, order:int=1):\n",
    "    # if node.type not in nodes:\n",
    "    #     return None\n",
    "    # if node.type == 'block':\n",
    "    #     for child in node.children:\n",
    "    #         node_to_dict(child, source_code, order)\n",
    "    children = [node_to_dict(child, source_code, i+1) for i, child in enumerate(node.children)]\n",
    "    children = [child for child in children if child is not None]\n",
    "    return {\n",
    "        \"type\": node.type,\n",
    "        \"start_byte\": node.start_byte,\n",
    "        \"end_byte\": node.end_byte,\n",
    "        \"order\": order,\n",
    "        \"text\": source_code[node.start_byte:node.end_byte].decode('utf8'),\n",
    "        \"children\": children\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6fdfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([None, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70e8b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = \"\"\"def main():\n",
    "    print(\"Hello, world!\")\n",
    "\"\"\"\n",
    "\n",
    "tree = py_parser.parse(code.encode(\"utf-8\"))\n",
    "tree_dict = node_to_dict(tree.root_node, code.encode(\"utf-8\"))\n",
    "tree_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d6ae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ingestion.py', 'rb') as file:\n",
    "    ingest_code = file.read()\n",
    "\n",
    "\n",
    "tree = py_parser.parse(ingest_code)\n",
    "tree_dict = node_to_dict(tree.root_node, ingest_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4dc908",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dict['children']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209d61f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tree_dict['children'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00de18f1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7a8900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from tree_sitter import Parser, Language\n",
    "import tree_sitter_rust\n",
    "\n",
    "# Create a Language object first\n",
    "RS_LANGUAGE = Language(tree_sitter_rust.language())\n",
    "rs_parser = Parser(RS_LANGUAGE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aa6385",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = json.loads(Path(\"node-types.json\").read_text())\n",
    "grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4952d07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = []\n",
    "\n",
    "for parent in grammar:\n",
    "    if 'subtypes' in parent:\n",
    "        if parent['type'] not in nodes:\n",
    "            nodes.append(parent['type'])\n",
    "        # if parent['type'] == '_compound_statement':\n",
    "        for child in parent['subtypes']:\n",
    "            if child['type'] not in nodes:\n",
    "                nodes.append(child['type'])\n",
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51887094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_to_dict(node, source_code, order:int=1):\n",
    "    # if node.type not in nodes:\n",
    "    #     return None\n",
    "    # if node.type == 'block':\n",
    "    #     for child in node.children:\n",
    "    #         node_to_dict(child, source_code, order)\n",
    "    children = [node_to_dict(child, source_code, i+1) for i, child in enumerate(node.children)]\n",
    "    children = [child for child in children if child is not None]\n",
    "    return {\n",
    "        \"type\": node.type,\n",
    "        \"start_byte\": node.start_byte,\n",
    "        \"end_byte\": node.end_byte,\n",
    "        \"order\": order,\n",
    "        \"text\": source_code[node.start_byte:node.end_byte].decode('utf8'),\n",
    "        \"children\": children\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4038b2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = \"\"\"use heed3::{types::*, Database, Env, RoTxn, RwTxn};\n",
    "use serde::{Deserialize, Serialize};\n",
    "use std::{borrow::Cow, collections::HashMap};\n",
    "\n",
    "use crate::{\n",
    "    helix_engine::{\n",
    "        storage_core::storage_core::HelixGraphStorage,\n",
    "        types::GraphError,\n",
    "        vector_core::{hnsw::HNSW, vector::HVector},\n",
    "    },\n",
    "    protocol::value::Value,\n",
    "};\n",
    "\n",
    "const DB_BM25_INVERTED_INDEX: &str = \"bm25_inverted_index\"; // term -> list of (doc_id, tf)\n",
    "const DB_BM25_DOC_LENGTHS: &str = \"bm25_doc_lengths\"; // doc_id -> document length\n",
    "const DB_BM25_TERM_FREQUENCIES: &str = \"bm25_term_frequencies\"; // term -> document frequency\n",
    "const DB_BM25_METADATA: &str = \"bm25_metadata\"; // stores total docs, avgdl, etc.\n",
    "\n",
    "#[derive(Serialize, Deserialize, Clone, Debug)]\n",
    "pub struct BM25Metadata {\n",
    "    pub total_docs: u64,\n",
    "    pub avgdl: f64,\n",
    "    pub k1: f32,\n",
    "    pub b: f32,\n",
    "}\n",
    "\n",
    "#[derive(Serialize, Deserialize, Clone, Debug)]\n",
    "pub struct PostingListEntry {\n",
    "    pub doc_id: u128,\n",
    "    pub term_frequency: u32,\n",
    "}\n",
    "\n",
    "pub trait BM25 {\n",
    "    fn tokenize<const SHOULD_FILTER: bool>(&self, text: &str) -> Vec<Cow<'_, str>>;\n",
    "    fn insert_doc(&self, txn: &mut RwTxn, doc_id: u128, doc: &str) -> Result<(), GraphError>;\n",
    "    fn update_doc(&self, txn: &mut RwTxn, doc_id: u128, doc: &str) -> Result<(), GraphError>;\n",
    "    fn delete_doc(&self, txn: &mut RwTxn, doc_id: u128) -> Result<(), GraphError>;\n",
    "    fn search(\n",
    "        &self,\n",
    "        txn: &RoTxn,\n",
    "        query: &str,\n",
    "        limit: usize,\n",
    "    ) -> Result<Vec<(u128, f32)>, GraphError>;\n",
    "    fn calculate_bm25_score(\n",
    "        &self,\n",
    "        term: &str,\n",
    "        doc_id: u128,\n",
    "        tf: u32,\n",
    "        doc_length: u32,\n",
    "        df: u32,\n",
    "        total_docs: u64,\n",
    "        avgdl: f64,\n",
    "    ) -> f32;\n",
    "}\n",
    "\n",
    "pub struct HBM25Config {\n",
    "    pub graph_env: Env,\n",
    "    pub inverted_index_db: Database<Bytes, Bytes>,\n",
    "    pub doc_lengths_db: Database<U128<heed3::byteorder::BE>, U32<heed3::byteorder::BE>>,\n",
    "    pub term_frequencies_db: Database<Bytes, U32<heed3::byteorder::BE>>,\n",
    "    pub metadata_db: Database<Bytes, Bytes>,\n",
    "}\n",
    "\n",
    "impl HBM25Config {\n",
    "    pub fn new(graph_env: &Env, wtxn: &mut RwTxn) -> Result<HBM25Config, GraphError> {\n",
    "        let inverted_index_db: Database<Bytes, Bytes> = graph_env\n",
    "            .database_options()\n",
    "            .types::<Bytes, Bytes>()\n",
    "            .flags(heed3::DatabaseFlags::DUP_SORT)\n",
    "            .name(DB_BM25_INVERTED_INDEX)\n",
    "            .create(wtxn)?;\n",
    "\n",
    "        let doc_lengths_db: Database<U128<heed3::byteorder::BE>, U32<heed3::byteorder::BE>> =\n",
    "            graph_env\n",
    "                .database_options()\n",
    "                .types::<U128<heed3::byteorder::BE>, U32<heed3::byteorder::BE>>()\n",
    "                .name(DB_BM25_DOC_LENGTHS)\n",
    "                .create(wtxn)?;\n",
    "\n",
    "        let term_frequencies_db: Database<Bytes, U32<heed3::byteorder::BE>> = graph_env\n",
    "            .database_options()\n",
    "            .types::<Bytes, U32<heed3::byteorder::BE>>()\n",
    "            .name(DB_BM25_TERM_FREQUENCIES)\n",
    "            .create(wtxn)?;\n",
    "\n",
    "        let metadata_db: Database<Bytes, Bytes> = graph_env\n",
    "            .database_options()\n",
    "            .types::<Bytes, Bytes>()\n",
    "            .name(DB_BM25_METADATA)\n",
    "            .create(wtxn)?;\n",
    "\n",
    "        Ok(HBM25Config {\n",
    "            graph_env: graph_env.clone(),\n",
    "            inverted_index_db,\n",
    "            doc_lengths_db,\n",
    "            term_frequencies_db,\n",
    "            metadata_db,\n",
    "        })\n",
    "    }\n",
    "}\n",
    "\n",
    "impl BM25 for HBM25Config {\n",
    "    fn tokenize<const SHOULD_FILTER: bool>(&self, text: &str) -> Vec<Cow<'_, str>> {\n",
    "        text.to_lowercase()\n",
    "            .replace(|c: char| !c.is_alphanumeric(), \" \")\n",
    "            .split_whitespace()\n",
    "            .filter(|s| !SHOULD_FILTER || s.len() > 2)\n",
    "            .map(|s| Cow::Owned(s.to_string()))\n",
    "            .collect()\n",
    "    }\n",
    "\n",
    "    fn insert_doc(&self, txn: &mut RwTxn, doc_id: u128, doc: &str) -> Result<(), GraphError> {\n",
    "        let tokens = self.tokenize::<true>(doc);\n",
    "        let doc_length = tokens.len() as u32;\n",
    "\n",
    "        let mut term_counts: HashMap<Cow<'_, str>, u32> = HashMap::new();\n",
    "        for token in tokens {\n",
    "            *term_counts.entry(token).or_insert(0) += 1;\n",
    "        }\n",
    "\n",
    "        self.doc_lengths_db.put(txn, &doc_id, &doc_length)?;\n",
    "\n",
    "        for (term, tf) in term_counts {\n",
    "            let term_bytes = term.as_bytes();\n",
    "\n",
    "            let posting_entry = PostingListEntry {\n",
    "                doc_id,\n",
    "                term_frequency: tf,\n",
    "            };\n",
    "\n",
    "            let posting_bytes = bincode::serialize(&posting_entry)?;\n",
    "\n",
    "            self.inverted_index_db\n",
    "                .put(txn, term_bytes, &posting_bytes)?;\n",
    "\n",
    "            let current_df = self.term_frequencies_db.get(txn, term_bytes)?.unwrap_or(0);\n",
    "            self.term_frequencies_db\n",
    "                .put(txn, term_bytes, &(current_df + 1))?;\n",
    "        }\n",
    "        let metadata_key = b\"metadata\";\n",
    "        let mut metadata = if let Some(data) = self.metadata_db.get(txn, metadata_key)? {\n",
    "            bincode::deserialize::<BM25Metadata>(data)?\n",
    "        } else {\n",
    "            BM25Metadata {\n",
    "                total_docs: 0,\n",
    "                avgdl: 0.0,\n",
    "                k1: 1.2,\n",
    "                b: 0.75,\n",
    "            }\n",
    "        };\n",
    "\n",
    "        let old_total_docs = metadata.total_docs;\n",
    "        metadata.total_docs += 1;\n",
    "        metadata.avgdl = (metadata.avgdl * old_total_docs as f64 + doc_length as f64)\n",
    "            / metadata.total_docs as f64;\n",
    "\n",
    "        let metadata_bytes = bincode::serialize(&metadata)?;\n",
    "        self.metadata_db.put(txn, metadata_key, &metadata_bytes)?;\n",
    "\n",
    "        // txn.commit()?;\n",
    "        Ok(())\n",
    "    }\n",
    "\n",
    "    fn update_doc(&self, txn: &mut RwTxn, doc_id: u128, doc: &str) -> Result<(), GraphError> {\n",
    "        // For simplicity, delete and re-insert\n",
    "        self.delete_doc(txn, doc_id)?;\n",
    "        self.insert_doc(txn, doc_id, doc)\n",
    "    }\n",
    "\n",
    "    fn delete_doc(&self, txn: &mut RwTxn, doc_id: u128) -> Result<(), GraphError> {\n",
    "        let terms_to_update = {\n",
    "            let mut terms = Vec::new();\n",
    "            let mut iter = self.inverted_index_db.iter(txn)?;\n",
    "\n",
    "            while let Some((term_bytes, posting_bytes)) = iter.next().transpose()? {\n",
    "                let posting: PostingListEntry = bincode::deserialize(posting_bytes)?;\n",
    "                if posting.doc_id == doc_id {\n",
    "                    terms.push(term_bytes.to_vec());\n",
    "                }\n",
    "            }\n",
    "            terms\n",
    "        };\n",
    "\n",
    "        // Remove postings and update term frequencies\n",
    "        for term_bytes in terms_to_update {\n",
    "            // Collect entries to keep\n",
    "            let entries_to_keep = {\n",
    "                let mut entries = Vec::new();\n",
    "                if let Some(duplicates) = self.inverted_index_db.get_duplicates(txn, &term_bytes)? {\n",
    "                    for result in duplicates {\n",
    "                        let (_, posting_bytes) = result?;\n",
    "                        let posting: PostingListEntry = bincode::deserialize(posting_bytes)?;\n",
    "                        if posting.doc_id != doc_id {\n",
    "                            entries.push(posting_bytes.to_vec());\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "                entries\n",
    "            };\n",
    "\n",
    "            // Delete all entries for this term\n",
    "            self.inverted_index_db.delete(txn, &term_bytes)?;\n",
    "\n",
    "            // Re-add the entries we want to keep\n",
    "            for entry_bytes in entries_to_keep {\n",
    "                self.inverted_index_db.put(txn, &term_bytes, &entry_bytes)?;\n",
    "            }\n",
    "\n",
    "            // Update document frequency\n",
    "            let current_df = self.term_frequencies_db.get(txn, &term_bytes)?.unwrap_or(0);\n",
    "            if current_df > 0 {\n",
    "                self.term_frequencies_db\n",
    "                    .put(txn, &term_bytes, &(current_df - 1))?;\n",
    "            }\n",
    "        }\n",
    "\n",
    "        // Get document length before deleting it\n",
    "        let doc_length = self.doc_lengths_db.get(txn, &doc_id)?.unwrap_or(0);\n",
    "\n",
    "        self.doc_lengths_db.delete(txn, &doc_id)?;\n",
    "\n",
    "        // Update metadata\n",
    "        let metadata_key = b\"metadata\";\n",
    "        let metadata_data = self\n",
    "            .metadata_db\n",
    "            .get(txn, metadata_key)?\n",
    "            .map(|data| data.to_vec());\n",
    "\n",
    "        if let Some(data) = metadata_data {\n",
    "            let mut metadata: BM25Metadata = bincode::deserialize(&data)?;\n",
    "            if metadata.total_docs > 0 {\n",
    "                // Update average document length\n",
    "                metadata.avgdl = if metadata.total_docs > 1 {\n",
    "                    (metadata.avgdl * metadata.total_docs as f64 - doc_length as f64)\n",
    "                        / (metadata.total_docs - 1) as f64\n",
    "                } else {\n",
    "                    0.0\n",
    "                };\n",
    "                metadata.total_docs -= 1;\n",
    "\n",
    "                let metadata_bytes = bincode::serialize(&metadata)?;\n",
    "                self.metadata_db.put(txn, metadata_key, &metadata_bytes)?;\n",
    "            }\n",
    "        }\n",
    "\n",
    "        Ok(())\n",
    "    }\n",
    "\n",
    "    fn search(\n",
    "        &self,\n",
    "        txn: &RoTxn,\n",
    "        query: &str,\n",
    "        limit: usize,\n",
    "    ) -> Result<Vec<(u128, f32)>, GraphError> {\n",
    "        let query_terms = self.tokenize::<true>(query);\n",
    "        let mut doc_scores: HashMap<u128, f32> = HashMap::with_capacity(limit);\n",
    "\n",
    "        let metadata_key = b\"metadata\";\n",
    "        let metadata = self\n",
    "            .metadata_db\n",
    "            .get(txn, metadata_key)?\n",
    "            .ok_or(GraphError::New(\"BM25 metadata not found\".to_string()))?;\n",
    "        let metadata: BM25Metadata = bincode::deserialize(metadata)?;\n",
    "\n",
    "        // For each query term, calculate scores\n",
    "        for term in query_terms {\n",
    "            let term_bytes = term.as_bytes();\n",
    "\n",
    "            // Get document frequency for this term\n",
    "            let df = self.term_frequencies_db.get(txn, term_bytes)?.unwrap_or(0);\n",
    "            // if df == 0 {\n",
    "            //     continue; // Term not in index\n",
    "            // }\n",
    "\n",
    "            // Get all documents containing this term\n",
    "            if let Some(duplicates) = self.inverted_index_db.get_duplicates(txn, term_bytes)? {\n",
    "                for result in duplicates {\n",
    "                    let (_, posting_bytes) = result?;\n",
    "                    let posting: PostingListEntry = bincode::deserialize(posting_bytes)?;\n",
    "\n",
    "                    // Get document length\n",
    "                    let doc_length = self.doc_lengths_db.get(txn, &posting.doc_id)?.unwrap_or(0);\n",
    "\n",
    "                    // Calculate BM25 score for this term in this document\n",
    "                    let score = self.calculate_bm25_score(\n",
    "                        &term,\n",
    "                        posting.doc_id,\n",
    "                        posting.term_frequency,\n",
    "                        doc_length,\n",
    "                        df,\n",
    "                        metadata.total_docs,\n",
    "                        metadata.avgdl,\n",
    "                    );\n",
    "\n",
    "                    // Add to document's total score\n",
    "                    *doc_scores.entry(posting.doc_id).or_insert(0.0) += score;\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        // Sort by score and return top results\n",
    "        let mut results: Vec<(u128, f32)> = doc_scores.into_iter().collect();\n",
    "        results.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));\n",
    "        results.truncate(limit);\n",
    "\n",
    "        Ok(results)\n",
    "    }\n",
    "\n",
    "    fn calculate_bm25_score(\n",
    "        &self,\n",
    "        _term: &str,\n",
    "        _doc_id: u128,\n",
    "        tf: u32,\n",
    "        doc_length: u32,\n",
    "        df: u32,\n",
    "        total_docs: u64,\n",
    "        avgdl: f64,\n",
    "    ) -> f32 {\n",
    "        let k1 = 1.2;\n",
    "        let b = 0.75;\n",
    "\n",
    "        // Ensure we don't have division by zero\n",
    "        let df = df.max(1);\n",
    "        let total_docs = total_docs.max(1);\n",
    "\n",
    "        // Calculate IDF: log((N - df + 0.5) / (df + 0.5))\n",
    "        // This can be negative when df is high relative to N, which is mathematically correct\n",
    "        let idf = ((total_docs as f64 - df as f64 + 0.5) / (df as f64 + 0.5)).ln();\n",
    "\n",
    "        // Ensure avgdl is not zero\n",
    "        let avgdl = if avgdl > 0.0 {\n",
    "            avgdl\n",
    "        } else {\n",
    "            doc_length as f64\n",
    "        };\n",
    "\n",
    "        // Calculate BM25 score\n",
    "        let tf_component = (tf as f64 * (k1 as f64 + 1.0))\n",
    "            / (tf as f64 + k1 as f64 * (1.0 - b as f64 + b as f64 * (doc_length as f64 / avgdl)));\n",
    "\n",
    "        let score = (idf * tf_component) as f32;\n",
    "\n",
    "        // The score can be negative when IDF is negative (term appears in most documents)\n",
    "        // This is mathematically correct - such terms have low discriminative power\n",
    "        // But documents with higher tf should still score higher than those with lower tf\n",
    "        score\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "tree = rs_parser.parse(code.encode(\"utf-8\"))\n",
    "tree_dict = node_to_dict(tree.root_node, code.encode(\"utf-8\"))\n",
    "tree_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811bcefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for child in tree_dict['children']:\n",
    "    if child['type'] not in nodes:\n",
    "        print(child['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43a522c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'line_comment' in nodes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
